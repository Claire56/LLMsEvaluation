"""
ROUGE and BLEU metric implementations for text evaluation.

ROUGE (Recall-Oriented Understudy for Gisting Evaluation):
- Measures overlap between generated and reference text
- Focuses on recall (what information was captured)
- Best for summarization tasks

BLEU (Bilingual Evaluation Understudy):
- Measures precision of n-grams
- Focuses on precision (how accurate the generation is)
- Best for translation tasks
"""

from typing import Dict, List
from rouge_score import rouge_scorer
from sacrebleu import BLEU
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)


def calculate_rouge(
    generated_text: str,
    reference_text: str,
    rouge_types: List[str] = ['rouge1', 'rouge2', 'rougeL']
) -> Dict[str, float]:
    """
    Calculate ROUGE scores between generated and reference text.
    
    Args:
        generated_text: The text generated by the LLM
        reference_text: The reference/ground truth text
        rouge_types: Types of ROUGE scores to calculate
        
    Returns:
        Dictionary with ROUGE scores (rouge1, rouge2, rougeL)
        
    Example:
        >>> gen = "The cat sat on the mat."
        >>> ref = "A cat was sitting on a mat."
        >>> scores = calculate_rouge(gen, ref)
        >>> print(scores['rouge1'])  # Unigram overlap
    """
    scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)
    scores = scorer.score(reference_text, generated_text)
    
    result = {}
    for rouge_type in rouge_types:
        # Extract F1 score (harmonic mean of precision and recall)
        result[rouge_type] = scores[rouge_type].fmeasure
    
    return result


def calculate_bleu(
    generated_text: str,
    reference_text: str,
    max_n: int = 4
) -> Dict[str, float]:
    """
    Calculate BLEU score between generated and reference text.
    
    Args:
        generated_text: The text generated by the LLM
        reference_text: The reference/ground truth text
        max_n: Maximum n-gram order (typically 4)
        
    Returns:
        Dictionary with BLEU score and breakdown by n-gram
        
    Example:
        >>> gen = "The cat sat on the mat."
        >>> ref = "A cat was sitting on a mat."
        >>> scores = calculate_bleu(gen, ref)
        >>> print(scores['bleu'])  # Overall BLEU score
    """
    # Tokenize texts
    gen_tokens = word_tokenize(generated_text.lower())
    ref_tokens = word_tokenize(reference_text.lower())
    
    # BLEU expects list of references (can have multiple references)
    references = [ref_tokens]
    hypothesis = gen_tokens
    
    # Calculate BLEU score
    bleu = BLEU(max_ngram_order=max_n)
    score = bleu.corpus_score([hypothesis], [references])
    
    return {
        'bleu': score.score / 100.0,  # Convert to 0-1 scale
        'precisions': {
            f'p{i+1}': p / 100.0 
            for i, p in enumerate(score.precisions)
        },
        'brevity_penalty': score.bp
    }


def calculate_rouge_bleu_summary(
    generated_text: str,
    reference_text: str
) -> Dict[str, float]:
    """
    Calculate both ROUGE and BLEU scores for comprehensive evaluation.
    
    Args:
        generated_text: The text generated by the LLM
        reference_text: The reference/ground truth text
        
    Returns:
        Dictionary with all ROUGE and BLEU metrics
    """
    rouge_scores = calculate_rouge(generated_text, reference_text)
    bleu_scores = calculate_bleu(generated_text, reference_text)
    
    return {
        **rouge_scores,
        **bleu_scores,
        'rouge_avg': sum(rouge_scores.values()) / len(rouge_scores)
    }

